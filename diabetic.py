# -*- coding: utf-8 -*-
"""Diabetic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QEetBNEfbpH5z34PX8e-4DVLiRHIgB7j

**Importing Libraries **
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn

import seaborn as sns

"""Importing Dataset"""

Data = pd.read_csv('diabetes.csv')

"""## EDA"""

Data.head()

Data.describe()

"""Checking the Target values to check if it data is balanced or unbalanced """

Data.Outcome.value_counts()

"""Checking the Null Values"""

Data.isnull().sum()

"""Checking the count of zerro enteries in each column"""

def Zerrovaluecount(column):
  for x in column:
    output = Data[x][Data[x] ==0].count()
    print(f'Count of zerro entries in {x} is {output}')

column = (Data.columns)
 Zerrovaluecount(column)

"""Glucose level, BloodPressure and BMI can never be zerro , so these are incorrcect entries so need to impute them.Here we are replacing them with mode/most frequent value"""

Data.Glucose.replace(0, Data.Glucose.mode().mean(), inplace=True)
Data.BloodPressure.replace(0, Data.BloodPressure.mode().mean(), inplace=True)
Data.BMI.replace(0, Data.BMI.mode().mean(), inplace=True)

Zerrovaluecount(column)

Data.describe()

"""Checking the Correlation between the features"""

corrMatrix = Data.corr()
sns.heatmap(corrMatrix, annot=True)

"""# Model Creation"""

X = Data.iloc[:, :-1].values
y = Data.iloc[:, -1].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1, stratify =Data['Outcome'] )

"""Using ensemble classifiers """

from sklearn.ensemble import RandomForestClassifier
RF = RandomForestClassifier(n_estimators = 500, min_samples_leaf = 2)
RF.fit(X_train,y_train)
print("Accuracy on training set: {:.3f}".format(RF.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(RF.score(X_test, y_test)))

"""Checking feature importance"""

print("Feature importances:\n{}".format(RF.feature_importances_))

def plot_feature_importances(model):
    n_features = 8
    plt.barh(range(n_features), RF.feature_importances_, align='center')
    
    plt.xlabel("Feature importance")
    plt.ylabel("Feature")
    plt.ylim(-1, n_features)
plot_feature_importances(RF)

"""For KNN and MLP we have to keep our dataset in same scale so we need scaling, here we will be using Robust scaler as its robust to outliers and suitable for all kind of dataset , we can not use Logistic regration as from pairplots we got an idea that data is not linearly seperable"""

from sklearn.preprocessing import RobustScaler
RS = RobustScaler()
X_train1 = RS.fit_transform(X_train)
X_test1 = RS.transform(X_test)

"""To check the optimum number of nearest neighbours """

from sklearn.neighbors import KNeighborsClassifier
training_accuracy = []
test_accuracy = []
# try n_neighbors from 1 to 10
neighbors_settings = range(1, 11)
for n_neighbors in neighbors_settings:
    # build the model
    knn = KNeighborsClassifier(n_neighbors=n_neighbors)
    knn.fit(X_train1, y_train)
    # record training set accuracy
    training_accuracy.append(knn.score(X_train1, y_train))
    # record test set accuracy
    test_accuracy.append(knn.score(X_test1, y_test))
plt.plot(neighbors_settings, training_accuracy, label="training accuracy")
plt.plot(neighbors_settings, test_accuracy, label="test accuracy")
plt.ylabel("Accuracy")
plt.xlabel("n_neighbors")
plt.legend()

"""we are getting good accuracy with 9 neighbours

"""

knn = KNeighborsClassifier(n_neighbors=9)
knn.fit(X_train, y_train)
print('Accuracy of K-NN classifier on training set: {:.3f}'.format(knn.score(X_train, y_train)))
print('Accuracy of K-NN classifier on test set: {:.3f}'.format(knn.score(X_test, y_test)))

"""MLP Classifier"""

from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(max_iter=1000, random_state=42, alpha=1)
mlp.fit(X_train1, y_train)
print("Accuracy on training set: {:.3f}".format(mlp.score(X_train1, y_train)))
print("Accuracy on test set: {:.3f}".format(mlp.score(X_test1, y_test)))

"""So , our randomForce and MLP model are giving good Accuracy but Random force seems to be overfit , thus we will be going with MPL classifier and creating the pipeline for MLP classifier(further we can also do hyperparameter tuning for MLP to get better accuracy)"""

from sklearn.pipeline import Pipeline
pipe = Pipeline([('scaler', RobustScaler()), ('MLP', mlp)])

pipe.fit(X_train,y_train)
print("Accuracy on training set: {:.3f}".format(pipe.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(pipe.score(X_test, y_test)))

import pickle
file_name = 'Diabetes1.sav'
pickle.dump(pipe,open(file_name, 'wb') )